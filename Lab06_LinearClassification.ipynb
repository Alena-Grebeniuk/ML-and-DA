{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alena-Grebeniuk/ML-and-DA/blob/main/Lab06_LinearClassification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4i0ChBBhxF0K"
      },
      "source": [
        "# Linear Classification\n",
        "\n",
        "In this lab you will implement parts of a linear classification model using the regularized empirical risk minimization principle. By completing this lab and analysing the code, you gain deeper understanding of these type of models, and of gradient descent.\n",
        "\n",
        "\n",
        "## Problem Setting\n",
        "\n",
        "The dataset describes diagnosing of cardiac Single Proton Emission Computed Tomography (SPECT) images. Each of the patients is classified into two categories: normal (1) and abnormal (0). The training data contains 80 SPECT images from which 22 binary features have been extracted. The goal is to predict the label for an unseen test set of 187 tomography images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "scrolled": true,
        "id": "qSGEB3UkxF0L"
      },
      "outputs": [],
      "source": [
        "import urllib.request\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "# for auto-reloading external modules\n",
        "# see http://stackoverflow.com/questions/1907993/autoreload-of-modules-in-ipython\n",
        "%load_ext autoreload\n",
        "%autoreload 2\n",
        "\n",
        "testfile = urllib.request.URLopener()\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.train\", \"SPECT.train\")\n",
        "testfile.retrieve(\"http://archive.ics.uci.edu/ml/machine-learning-databases/spect/SPECT.test\", \"SPECT.test\")\n",
        "\n",
        "df_train = pd.read_csv('SPECT.train',header=None)\n",
        "df_test = pd.read_csv('SPECT.test',header=None)\n",
        "\n",
        "train = df_train.values\n",
        "test = df_test.values\n",
        "\n",
        "y_train = train[:,0]\n",
        "X_train = train[:,1:]\n",
        "y_test = test[:,0]\n",
        "X_test = test[:,1:]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DBPhAtmexF0N"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Analyze the function learn_reg_ERM(X,y,lambda) which for a given $n\\times m$ data matrix $\\textbf{X}$ and binary class label $\\textbf{y}$ learns and returns a linear model $\\textbf{w}$.\n",
        "The binary class label has to be transformed so that its range is $\\left \\{-1,1 \\right \\}$.\n",
        "The trade-off parameter between the empirical loss and the regularizer is given by $\\lambda > 0$.\n",
        "To adapt the learning rate the Barzilai-Borwein method is used.\n",
        "\n",
        "Try to understand each step of the learning algorithm and comment each line.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "1-HgTxIQxF0N"
      },
      "outputs": [],
      "source": [
        "def learn_reg_ERM(X,y,lbda):\n",
        "    max_iter = 200\n",
        "    e  = 0.001\n",
        "    alpha = 1.\n",
        "\n",
        "    w = np.random.randn(X.shape[1]);\n",
        "    for k in np.arange(max_iter):\n",
        "        h = np.dot(X,w)\n",
        "        l,lg = loss(h, y)\n",
        "        print ('loss: {}'.format(np.mean(l)))\n",
        "        r,rg = reg(w, lbda)\n",
        "        g = np.dot(X.T,lg) + rg\n",
        "        if (k > 0):\n",
        "            alpha = alpha * (np.dot(g_old.T,g_old))/(np.dot((g_old - g).T,g_old))\n",
        "        w = w - alpha * g\n",
        "        if (np.linalg.norm(alpha * g) < e):\n",
        "            break\n",
        "        g_old = g\n",
        "    return w"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "This function seems to implement a regularized empirical risk minimization (ERM) algorithm with a linear model. Let's break down each step:\n",
        "\n",
        "1. `max_iter = 200`: Sets the maximum number of iterations for the optimization process. If the algorithm doesn't converge within these iterations, it stops.\n",
        "\n",
        "2. `e = 0.001`: Defines a threshold for the norm of the gradient. If the norm of the gradient falls below this threshold, the optimization process is considered converged, and the algorithm stops.\n",
        "\n",
        "3. `alpha = 1.`: Initializes the learning rate. It's a common practice to start with a default value.\n",
        "\n",
        "4. `w = np.random.randn(X.shape[1])`: Initializes the weight vector randomly. The dimensionality of the weight vector matches the number of features in the dataset.\n",
        "\n",
        "5. `for k in np.arange(max_iter):`: Starts a loop over the maximum number of iterations.\n",
        "\n",
        "6. `h = np.dot(X, w)`: Computes the linear combination of the input features and the weight vector, representing the predicted values.\n",
        "\n",
        "7. `l, lg = loss(h, y)`: Computes the loss and its gradient with respect to the predicted values `h` and the true labels `y`. This function seems to be external to the provided code.\n",
        "\n",
        "8. `r, rg = reg(w, lbda)`: Computes the regularization term and its gradient with respect to the weight vector `w` and the regularization parameter `lbda`. This function also seems to be external.\n",
        "\n",
        "9. `g = np.dot(X.T, lg) + rg`: Computes the total gradient of the loss function by summing the gradient of the loss term and the gradient of the regularization term.\n",
        "\n",
        "10. `if (k > 0): ...`: This block adapts the learning rate `alpha` using the Barzilai-Borwein method. This method adjusts the learning rate based on the gradient descent steps taken so far.\n",
        "\n",
        "11. `w = w - alpha * g`: Updates the weight vector using gradient descent.\n",
        "\n",
        "12. `if (np.linalg.norm(alpha * g) < e): break`: Checks if the norm of the gradient times the learning rate falls below the threshold `e`. If it does, the algorithm terminates as it indicates convergence.\n",
        "\n",
        "13. `g_old = g`: Stores the gradient for the next iteration.\n",
        "\n",
        "14. `return w`: Returns the learned weight vector, which represents the linear model.\n",
        "\n",
        "Overall, this function implements a regularized empirical risk minimization algorithm with gradient descent, where the learning rate is adapted using the Barzilai-Borwein method, and termination occurs when the gradient falls below a certain threshold."
      ],
      "metadata": {
        "id": "d1DWAmBYuUJ6"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1CmQjDhxF0O"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Fill in the code for the function loss(h,y) which computes the hinge loss and its gradient.\n",
        "This function takes a given vector $\\textbf{y}$ with the true labels $\\in \\left \\{-1,1\\right \\}$ and a vector $\\textbf{h}$ with the function values of the linear model as inputs. The function returns a vector $\\textbf{l}$ with the hinge loss $\\max(0, 1 − y_{i} h_{i})$ and a vector $\\textbf{g}$ with the gradients of the hinge loss w.r.t $\\textbf{h}$. (Note: The partial derivative of the hinge loss with respect to $\\textbf{h}$  is $g_{i} = −y $ if $l_{i} > 0$, else $g_{i} = 0$)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def loss(h, y):\n",
        "    l = np.maximum(0, 1 - y * h)  # hinge loss\n",
        "    g = -y * (l > 0)             # gradient of hinge loss\n",
        "    return l, g\n"
      ],
      "metadata": {
        "id": "GdGSO32Bu-13"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "l = np.maximum(0, 1 - y * h): This line computes the hinge loss element-wise. It takes the maximum of 0 and 1 - y * h. If y * h >= 1, the hinge loss is 0; otherwise, it's 1 - y * h.\n",
        "\n",
        "g = -y * (l > 0): This line computes the gradient of the hinge loss. If l > 0, meaning the hinge loss is non-zero, the gradient is -y. Otherwise, it's 0.\n",
        "\n",
        "This function computes both the hinge loss l and its gradient g and returns them."
      ],
      "metadata": {
        "id": "68tWVZ_XvPle"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmdwcNAaxF0P"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Fill in the code for the function reg(w,lambda) which computes the $\\mathcal{L}_2$-regularizer and the gradient of the regularizer function at point $\\textbf{w}$.\n",
        "\n",
        "\n",
        "$$r = \\frac{\\lambda}{2} \\textbf{w}^{T}\\textbf{w}$$\n",
        "\n",
        "$$g = \\lambda \\textbf{w}$$"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def reg(w, lbda):\n",
        "    r = 0.5 * lbda * np.dot(w.T, w)  # ℓ2-regularizer\n",
        "    g = lbda * w                      # Gradient of ℓ2-regularizer\n",
        "    return r, g\n"
      ],
      "metadata": {
        "id": "z67bwO_WwPTI"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "r = 0.5 * lbda * np.dot(w.T, w): This line computes the ℓ2-regularizer. It calculates 0.5 times λ times the dot product of w with itself (transposed). This is the squared ℓ2-norm of w, scaled by λ.\n",
        "\n",
        "g = lbda * w: This line computes the gradient of the ℓ2-regularizer. It's simply λ times w, as the derivative of 0.5 * λ * ||w||² with respect to w is just λw.\n",
        "\n",
        "This function computes both the ℓ2-regularizer r and its gradient g and returns them."
      ],
      "metadata": {
        "id": "uKNOVAhbwtO4"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HXlyhFPmxF0Q"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Fill in the code for the function predict(w,x) which predicts the class label $y$ for a data point $\\textbf{x}$ or a matrix $X$ of data points (row-wise) for a previously trained linear model $\\textbf{w}$. If there is only a data point given, the function is supposed to return a scalar value. If a matrix is given a vector of predictions is supposed to be returned."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def predict(w, X):\n",
        "    preds = np.sign(np.dot(X, w))  # Predictions: 1 if dot product >= 0, -1 otherwise\n",
        "    return preds\n"
      ],
      "metadata": {
        "id": "8AhLg_tayAOa"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Explanation:\n",
        "\n",
        "np.dot(X, w): This computes the dot product of the data points (X) with the weight vector (w). For a single data point, this results in a scalar; for a matrix of data points, this results in a vector of dot products.\n",
        "\n",
        "np.sign(...): This function returns the sign of each element in the input array. So, if the dot product is positive or zero, it returns 1; otherwise, it returns -1.\n",
        "\n",
        "This function returns the predictions for the given data points based on the previously trained linear model\n",
        "�\n",
        "w."
      ],
      "metadata": {
        "id": "pNdjFJmSyDch"
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.10"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}